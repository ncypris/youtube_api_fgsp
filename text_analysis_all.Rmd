---
title: 'Text Analysis: Frequencies'
author: "Niklas Cypris"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(quanteda)
library(quanteda.textplots)

theme_set(theme_classic())
```

```{r, warning=FALSE}
load("data/vid1.Rda")
load("data/vid2.Rda")
```

# Possible Analyses

# Meta Data

## Time of Publishing

```{r}
barbie_comments %>% 
  ggplot(aes(x = publishedAt))+
  geom_histogram(bins = 90)
```

## Comments per User

```{r}
barbie_comments %>% 
  count(authorDisplayName, sort = TRUE)
```

## Likes for Comments

```{r}
barbie_comments %>% 
  ggplot(aes(x = likeCount))+
  geom_histogram()
```

```{r}
barbie_comments %>% 
  pull(likeCount) %>% 
  table()
```

## Replies to Posts

```{r}
barbie_comments %>% 
  filter(!is.na(parentId)) %>% 
  count(parentId) %>% 
  rename(id = parentId,
         n_replies = n) %>% 
  right_join(barbie_comments, by = join_by(id))
```

# Exercise

Check out the meta data for the comments to the Oppenheimer movie.

```{r}
oppenheimer_comments %>% 
  ggplot(aes(x = publishedAt))+
  geom_histogram(bins = 90)
```

### Comments per User

```{r}
oppenheimer_comments %>% 
  count(authorDisplayName, sort = TRUE)
```

### Likes for Comments

```{r}
oppenheimer_comments %>% 
  ggplot(aes(x = likeCount))+
  geom_histogram()
```

```{r}
oppenheimer_comments %>% 
  pull(likeCount) %>% 
  table()
```

### Replies to Posts

```{r}
oppenheimer_comments %>%  
  filter(!is.na(parentId)) %>% 
  count(parentId) %>% 
  rename(id = parentId,
         n_replies = n) %>% 
  right_join(oppenheimer_comments, by = join_by(id))
```

# Textual Anaylsis

## Frequently-used Words

```{r}
corp_barbie <- corpus(barbie_comments, text_field = "textDisplay")
```

```{r}
print(corp_barbie)
```

```{r}
toks_barbie <- corp_barbie %>% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE, include_docvars = TRUE) %>% 
  # remove stop words
  tokens_remove(pattern = stopwords("en")) %>% 
  tokens_remove(pattern = stopwords("de"))
```

*Remove special characters, keep emojis, and remove single letters:*

```{r}
pattern_to_remove <- "[^a-zA-Z0-9\\s\\x{1F600}-\\x{1F64F}\\x{1F300}-\\x{1F5FF}\\x{1F680}-\\x{1F6FF}\\x{1F700}-\\x{1F77F}\\x{1F780}-\\x{1F7FF}\\x{1F800}-\\x{1F8FF}\\x{1F900}-\\x{1F9FF}\\x{1FA00}-\\x{1FA6F}\\x{1FA70}-\\x{1FAFF}]|\\b[a-zA-Z]\\b"
```

**Special Letters**

The pattern "[^a-zA-Z0-9\\s]" matches any character that's not:
a lowercase letter (a-z)
an uppercase letter (A-Z)
a number (0-9)
or a whitespace (\\s)

**Emojis**

\x{1F600}-\x{1F64F} represents the range of emojis in the "emoticons" block of Unicode.

The other \x{...} ranges represent other blocks of emojis in the Unicode standard.

**Single Letters**

\\b[a-zA-Z]\\b matches single letters. Here, \\b is a word boundary anchor ensuring that it matches a single letter as a whole word.

```{r}
toks_barbie <- toks_barbie %>% 
  tokens_remove(pattern = pattern_to_remove, valuetype = "regex") %>% 
  tokens_remove(pattern = c("href", "quot", "amp", "br")) # removing other unnecessary words
```

```{r}
print(toks_barbie)
```

```{r}
toks_2gram_barbie <- tokens_ngrams(toks_barbie, n = 2)
toks_3gram_barbie <- tokens_ngrams(toks_barbie, n = 3)
```

```{r}
print(toks_2gram_barbie)
```

```{r}
dfm_barbie <- dfm(toks_barbie)
dfm_2gram_barbie <- dfm(toks_2gram_barbie)
dfm_3gram_barbie <- dfm(toks_3gram_barbie)
```

```{r}
print(dfm_barbie)
```

```{r}
topfeatures(dfm_barbie, 30)
```

```{r}
topfeatures(dfm_2gram_barbie, 30)
```

```{r}
topfeatures(dfm_3gram_barbie, 30)
```

## Wordcloud

```{r}
textplot_wordcloud(dfm_barbie, max_words = 200)
```

# Exercise

Apply the above-mentioned steps for the Oppenheimer movie data set. Which words are the most prominent words? Is there any overlap between movies? How about overlap in the 2grams?

```{r}
corp_opp <- corpus(oppenheimer_comments, text_field = "textDisplay")
```

```{r}
print(corp_opp)
```

```{r}
toks_opp <- corp_opp %>% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE, include_docvars = TRUE) %>% 
  # remove stop words
  tokens_remove(pattern = stopwords("en")) %>% 
  tokens_remove(pattern = stopwords("de"))
```

```{r}
toks_opp <- toks_opp %>% 
  tokens_remove(pattern = pattern_to_remove, valuetype = "regex") %>% 
  tokens_remove(pattern = c("href", "quot", "amp", "br")) # removing other unnecessary words
```

```{r}
print(toks_opp)
```

```{r}
toks_2gram_opp <- tokens_ngrams(toks_opp, n = 2)
toks_3gram_opp <- tokens_ngrams(toks_opp, n = 3)
```

```{r}
print(toks_2gram_opp)
```

```{r}
dfm_opp <- dfm(toks_opp)
dfm_2gram_opp <- dfm(toks_2gram_opp)
dfm_3gram_opp <- dfm(toks_3gram_opp)
```

```{r}
print(dfm_opp)
```

```{r}
topfeatures(dfm_opp, 30)
```

```{r}
topfeatures(dfm_2gram_opp, 30)
```

```{r}
topfeatures(dfm_3gram_opp, 30)
```

### Wordcloud

```{r}
textplot_wordcloud(dfm_opp, max_words = 200)
```

# Further Reading

https://tutorials.quanteda.io/












